#!/usr/bin/env python3
"""
Orchestrateur IA 4LB.ca - Backend FastAPI v3.1 (Avec Pont MCP)
Agent autonome avec boucle ReAct, s√©lection auto de mod√®le, upload fichiers
S√©curit√©: JWT, Rate Limiting, Validation commandes/chemins
"""

import os
import json
import asyncio
import sqlite3
import httpx
import subprocess
import re
import base64
import uuid
import shutil
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, UploadFile, File, Form, Depends, Request, Query
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import OAuth2PasswordRequestForm
from pydantic import BaseModel
import chromadb
from chromadb.config import Settings

# ===== MODULES DE S√âCURIT√â v3.0 =====
try:
    from security import (
        validate_command,
        validate_path,
        is_path_allowed,
        CommandNotAllowedError,
        PathNotAllowedError,
        audit_log,
        get_security_config,
    )
    SECURITY_ENABLED = True
except ImportError:
    SECURITY_ENABLED = False
    print("‚ö†Ô∏è Module security non disponible - Validation d√©sactiv√©e")

try:
    from auth import (
        get_current_user,
        verify_token,
        get_current_active_user,
        get_optional_user,
        get_current_admin_user,
        require_scope,
        create_access_token,
        create_refresh_token,
        verify_refresh_token,
        revoke_refresh_token,
        authenticate_user,
        create_user,
        update_user,
        get_user,
        check_login_rate_limit,
        record_login_attempt,
        create_api_key,
        init_auth_db,
        Token,
        User,
        UserCreate,
        UserUpdate,
        APIKey,
        AUTH_ENABLED,
    )
except ImportError:
    AUTH_ENABLED = False
    print("‚ö†Ô∏è Module auth non disponible - Authentification d√©sactiv√©e")

try:
    from rate_limiter import RateLimitMiddleware, rate_limiter, get_rate_limit_stats, cleanup_task
    RATE_LIMIT_ENABLED = True
except ImportError:
    RATE_LIMIT_ENABLED = False
    print("‚ö†Ô∏è Module rate_limiter non disponible - Rate limiting d√©sactiv√©")

try:
    from config import get_settings, get_cors_config, MODELS as CONFIG_MODELS
    CONFIG_ENABLED = True
except ImportError:
    CONFIG_ENABLED = False
    print("‚ö†Ô∏è Module config non disponible - Configuration par d√©faut")

# ===== INT√âGRATION PONT MCP (Bridge) =====
# C'est ici que l'orchestrateur se connecte au serveur MCP (Port 8889)
try:
    from backend.mcp_bridge import mcp_run_command, mcp_read_file, mcp_write_file
    MCP_ENABLED = True
    print("‚úÖ PONT MCP ACTIV√â: L'orchestrateur utilisera le serveur MCP sur le port 8889")
except ImportError:
    try:
        # Fallback si le fichier est √† la racine
        from mcp_bridge import mcp_run_command, mcp_read_file, mcp_write_file
        MCP_ENABLED = True
        print("‚úÖ PONT MCP ACTIV√â (Racine)")
    except ImportError:
        MCP_ENABLED = False
        print("‚ö†Ô∏è PONT MCP NON D√âTECT√â: Utilisation des commandes locales (subprocess)")


# Helper pour protection optionnelle des endpoints
def optional_auth():
    """Retourne une d√©pendance d'auth si AUTH_ENABLED, sinon None"""
    if AUTH_ENABLED:
        return Depends(get_optional_user)
    return None


# Helper pour protection obligatoire des endpoints
def require_auth():
    """Retourne une d√©pendance d'auth obligatoire si AUTH_ENABLED"""
    if AUTH_ENABLED:
        return Depends(get_current_active_user)
    return None
# D√©sactiver la t√©l√©m√©trie ChromaDB pour √©viter l'erreur capture()
import os
os.environ["ANONYMIZED_TELEMETRY"] = "False"

# Patch posthog pour √©viter l'erreur "capture() takes 1 positional argument but 3 were given"
try:
    import posthog
    posthog.disabled = True
    # Remplacer capture par une fonction vide
    posthog.capture = lambda *args, **kwargs: None
except ImportError:
    pass

# Module d'auto-apprentissage
try:
    from auto_learn import (
        auto_learn_from_message,
        save_conversation_summary,
        get_relevant_context,
        get_user_preferences,
        get_memory_stats
    )
    AUTO_LEARN_ENABLED = True
except ImportError:
    AUTO_LEARN_ENABLED = False
    print("‚ö†Ô∏è Module auto_learn non disponible")

# ===== CONFIGURATION =====

OLLAMA_URL = os.getenv("OLLAMA_URL", "http://10.10.10.46:11434")
DB_PATH = "/data/orchestrator.db"
UPLOAD_DIR = "/data/uploads"
# ChromaDB pour m√©moire s√©mantique
CHROMADB_HOST = os.getenv("CHROMADB_HOST", "chromadb")
CHROMADB_PORT = int(os.getenv("CHROMADB_PORT", "8000"))

def get_chroma_client():
    """Obtenir le client ChromaDB"""
    try:
        client = chromadb.HttpClient(host=CHROMADB_HOST, port=CHROMADB_PORT)
        return client
    except Exception as e:
        print(f"Erreur ChromaDB: {e}")
        return None

def get_memory_collection():
    """Obtenir ou cr√©er la collection de m√©moire"""
    client = get_chroma_client()
    if client:
        return client.get_or_create_collection(
            name="ai_orchestrator_memory",
            metadata={"description": "M√©moire s√©mantique de l'AI Orchestrator"}
        )
    return None

MAX_ITERATIONS = 12

# Mod√®les disponibles avec leurs sp√©cialit√©s
MODELS = {
    "auto": {
        "name": "AUTO (S√©lection automatique)",
        "description": "L'agent choisit le meilleur mod√®le selon la t√¢che",
        "model": None
    },
    # === MOD√àLES LOCAUX ===
    "qwen-coder": {
        "name": "üíª Qwen 2.5 Coder 32B",
        "description": "Code, scripts, debug, analyse technique",
        "model": "qwen2.5-coder:32b-instruct-q4_K_M",
        "keywords": ["code", "script", "python", "bash", "debug", "fonction", "variable", "api", "docker", "git", "npm", "programm"]
    },
    "deepseek-coder": {
        "name": "üß† DeepSeek Coder 33B",
        "description": "Code alternatif, algorithmes complexes",
        "model": "deepseek-coder:33b",
        "keywords": ["algorithme", "optimis", "complex", "performance", "refactor"]
    },
    "llama-vision": {
        "name": "üëÅÔ∏è Llama 3.2 Vision 11B",
        "description": "Analyse d'images, OCR, vision",
        "model": "llama3.2-vision:11b-instruct-q8_0",
        "keywords": ["image", "photo", "screenshot", "capture", "voir", "regarde", "analyse visuel", "ocr"]
    },
    "qwen-vision": {
        "name": "üé® Qwen3 VL 32B",
        "description": "Vision multimodale avanc√©e",
        "model": "qwen3-vl:32b",
        "keywords": ["image", "multimodal", "vision", "graphique", "diagramme", "sch√©ma"]
    },
    # === MOD√àLES CLOUD (via Ollama) ===
    "kimi-k2": {
        "name": "‚òÅÔ∏è Kimi K2 1T",
        "description": "Mod√®le cloud Kimi (Moonshot AI)",
        "model": "kimi-k2:1t-cloud",
        "keywords": ["kimi", "moonshot", "cloud", "chinois"]
    },
    "qwen3-coder-cloud": {
        "name": "‚òÅÔ∏è Qwen3 Coder 480B",
        "description": "Qwen3 Coder g√©ant via cloud",
        "model": "qwen3-coder:480b-cloud",
        "keywords": ["qwen", "cloud", "coder", "gros"]
    },
    "gemini-pro": {
        "name": "‚òÅÔ∏è Gemini 3 Pro",
        "description": "Google Gemini Pro via cloud",
        "model": "gemini-3-pro-preview:latest",
        "keywords": ["gemini", "google", "cloud"]
    },
    "gpt-safeguard": {
        "name": "üõ°Ô∏è GPT Safeguard 13B",
        "description": "GPT Open Source local (s√©curit√©)",
        "model": "gpt-oss-safeguard:latest",
        "keywords": ["gpt", "safeguard", "s√©curit√©", "mod√©ration"]
    }
}

DEFAULT_MODEL = "qwen3-coder:480b-cloud"

# ===== D√âFINITION DES OUTILS =====

TOOLS = {
    "execute_command": {
        "description": "Ex√©cuter une commande bash sur le serveur Ubuntu",
        "parameters": {"command": "string - La commande √† ex√©cuter"},
        "example": "execute_command(command=\"ls -la /home/lalpha\")"
    },
    "system_info": {
        "description": "Obtenir les informations syst√®me (CPU, RAM, disque, GPU)",
        "parameters": {},
        "example": "system_info()"
    },
    "docker_status": {
        "description": "Voir l'√©tat de tous les conteneurs Docker",
        "parameters": {},
        "example": "docker_status()"
    },
    "docker_logs": {
        "description": "Voir les logs d'un conteneur Docker",
        "parameters": {"container": "string - Nom du conteneur", "lines": "int - Nombre de lignes (d√©faut: 50)"},
        "example": "docker_logs(container=\"traefik\", lines=100)"
    },
    "docker_restart": {
        "description": "Red√©marrer un conteneur Docker",
        "parameters": {"container": "string - Nom du conteneur"},
        "example": "docker_restart(container=\"traefik\")"
    },
    "disk_usage": {
        "description": "Analyser l'utilisation du disque",
        "parameters": {"path": "string - Chemin √† analyser (d√©faut: /)"},
        "example": "disk_usage(path=\"/home/lalpha\")"
    },
    "service_status": {
        "description": "V√©rifier le statut d'un service systemd",
        "parameters": {"service": "string - Nom du service"},
        "example": "service_status(service=\"ollama\")"
    },
    "service_control": {
        "description": "Contr√¥ler un service (start, stop, restart)",
        "parameters": {"service": "string - Nom du service", "action": "string - Action: start, stop, restart"},
        "example": "service_control(service=\"ollama\", action=\"restart\")"
    },
    "read_file": {
        "description": "Lire le contenu d'un fichier",
        "parameters": {"path": "string - Chemin du fichier"},
        "example": "read_file(path=\"/home/lalpha/projets/README.md\")"
    },
    "write_file": {
        "description": "√âcrire du contenu dans un fichier",
        "parameters": {"path": "string - Chemin du fichier", "content": "string - Contenu √† √©crire"},
        "example": "write_file(path=\"/home/lalpha/test.txt\", content=\"Hello\")"
    },
    "list_directory": {
        "description": "Lister le contenu d'un r√©pertoire",
        "parameters": {"path": "string - Chemin du r√©pertoire"},
        "example": "list_directory(path=\"/home/lalpha/projets\")"
    },
    "search_files": {
        "description": "Rechercher des fichiers par nom ou contenu",
        "parameters": {"pattern": "string - Motif de recherche", "path": "string - R√©pertoire de recherche", "content": "bool - Chercher dans le contenu"},
        "example": "search_files(pattern=\"*.py\", path=\"/home/lalpha/projets\")"
    },
    "udm_status": {
        "description": "Obtenir le statut du UDM-Pro UniFi",
        "parameters": {},
        "example": "udm_status()"
    },
    "udm_network_info": {
        "description": "Informations r√©seau du UDM-Pro (VLANs, clients)",
        "parameters": {},
        "example": "udm_network_info()"
    },
    "udm_clients": {
        "description": "Liste des clients connect√©s au r√©seau",
        "parameters": {},
        "example": "udm_clients()"
    },
    "network_scan": {
        "description": "Scanner les ports ouverts du serveur",
        "parameters": {},
        "example": "network_scan()"
    },
    "ollama_list": {
        "description": "Lister les mod√®les Ollama install√©s",
        "parameters": {},
        "example": "ollama_list()"
    },
    "ollama_run": {
        "description": "Ex√©cuter une requ√™te sur un mod√®le Ollama sp√©cifique",
        "parameters": {"model": "string - Nom du mod√®le", "prompt": "string - Prompt"},
        "example": "ollama_run(model=\"qwen2.5-coder:32b\", prompt=\"Hello\")"
    },
    "analyze_image": {
        "description": "Analyser une image upload√©e avec le mod√®le vision",
        "parameters": {"image_id": "string - ID de l'image upload√©e", "question": "string - Question sur l'image"},
        "example": "analyze_image(image_id=\"abc123\", question=\"Que vois-tu sur cette image?\")"
    },
    "analyze_file": {
        "description": "Analyser un fichier upload√©",
        "parameters": {"file_id": "string - ID du fichier upload√©"},
        "example": "analyze_file(file_id=\"abc123\")"
    },
    "create_script": {
        "description": "Cr√©er un script bash ex√©cutable",
        "parameters": {"path": "string - Chemin", "content": "string - Contenu du script"},
        "example": "create_script(path=\"/home/lalpha/scripts/test.sh\", content=\"#!/bin/bash\\necho OK\")"
    },
    "git_status": {
        "description": "Voir le statut git d'un r√©pertoire",
        "parameters": {"path": "string - Chemin du repo"},
        "example": "git_status(path=\"/home/lalpha/projets/ai-tools\")"
    },
    "git_diff": {
        "description": "Voir les diff√©rences non commit√©es dans un repo git",
        "parameters": {"path": "string - Chemin du repo"},
        "example": "git_diff(path=\"/home/lalpha/projets/ai-tools\")"
    },
    "git_log": {
        "description": "Voir l historique des commits git",
        "parameters": {"path": "string - Chemin du repo", "n": "int - Nombre de commits (d√©faut: 10)"},
        "example": "git_log(path=\"/home/lalpha/projets/ai-tools\", n=20)"
    },
    "git_commit": {
        "description": "Commiter les changements avec un message",
        "parameters": {"path": "string - Chemin du repo", "message": "string - Message de commit", "add_all": "bool - Ajouter tous les fichiers (d√©faut: true)"},
        "example": "git_commit(path=\"/home/lalpha/projets/ai-tools\", message=\"feat: nouvelle fonctionnalit√©\")"
    },
    "git_branch": {
        "description": "Lister ou changer de branche git",
        "parameters": {"path": "string - Chemin du repo", "branch": "string - Nom de branche (optionnel, pour switch)"},
        "example": "git_branch(path=\"/home/lalpha/projets/ai-tools\")"
    },
    "git_pull": {
        "description": "Tirer les changements du remote",
        "parameters": {"path": "string - Chemin du repo"},
        "example": "git_pull(path=\"/home/lalpha/projets/ai-tools\")"
    },
    "memory_store": {
        "description": "IMPORTANT: Stocker une information importante en m√©moire s√©mantique.",
        "parameters": {"key": "string", "value": "string"},
        "example": "memory_store(key=\"utilisateur\", value=\"...\")"
    },
    "memory_recall": {
        "description": "IMPORTANT: Rechercher dans la m√©moire s√©mantique.",
        "parameters": {"query": "string"},
        "example": "memory_recall(query=\"projets en cours\")"
    },
    "memory_stats": {
        "description": "Afficher les statistiques de la m√©moire",
        "parameters": {},
        "example": "memory_stats()"
    },
    "web_request": {
        "description": "Faire une requ√™te HTTP GET/POST",
        "parameters": {"url": "string - URL", "method": "string - GET ou POST", "data": "string - Donn√©es JSON pour POST"},
        "example": "web_request(url=\"http://localhost:8001/health\", method=\"GET\")"
    },
    "create_plan": {
        "description": "Cr√©er un plan d'ex√©cution d√©taill√©.",
        "parameters": {"task": "string"},
        "example": "create_plan(task=\"Cr√©er un site web\")"
    },
    "validate_step": {
        "description": "Valider qu'une √©tape du plan a √©t√© correctement ex√©cut√©e",
        "parameters": {"step_description": "string", "expected_result": "string"},
        "example": "validate_step(step_description=\"Cr√©er le dossier\", expected_result=\"Dossier existe\")"
    },
    "final_answer": {
        "description": "Fournir la r√©ponse finale √† l'utilisateur",
        "parameters": {"answer": "string - La r√©ponse compl√®te et structur√©e"},
        "example": "final_answer(answer=\"Voici le r√©sultat...\")"
    }
}

# ===== BASE DE DONN√âES =====

def init_db():
    """Initialiser la base SQLite"""
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    os.makedirs(UPLOAD_DIR, exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    
    # Table conversations avec historique complet
    c.execute('''CREATE TABLE IF NOT EXISTS conversations (
        id TEXT PRIMARY KEY,
        title TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )''')
    
    # Table messages
    c.execute('''CREATE TABLE IF NOT EXISTS messages (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        conversation_id TEXT,
        role TEXT,
        content TEXT,
        model_used TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY (conversation_id) REFERENCES conversations(id)
    )''')
    
    # Table m√©moire
    c.execute('''CREATE TABLE IF NOT EXISTS memory (
        key TEXT PRIMARY KEY,
        value TEXT,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )''')
    
    # Table fichiers upload√©s
    c.execute('''CREATE TABLE IF NOT EXISTS uploads (
        id TEXT PRIMARY KEY,
        filename TEXT,
        filepath TEXT,
        filetype TEXT,
        filesize INTEGER,
        conversation_id TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )''')
    
    conn.commit()
    conn.close()

def get_db():
    """Obtenir une connexion DB"""
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn

# ===== S√âLECTION AUTOMATIQUE DE MOD√àLE =====

def auto_select_model(message: str, has_image: bool = False) -> str:
    """Toujours utiliser le modele cloud sauf pour les images"""
    if has_image:
        return MODELS["llama-vision"]["model"]
    return DEFAULT_MODEL

# ===== GESTION DES FICHIERS =====

async def save_upload(file: UploadFile, conversation_id: str = None) -> dict:
    """Sauvegarder un fichier upload√©"""
    file_id = str(uuid.uuid4())[:8]
    
    # D√©terminer le type
    filename = file.filename or "unknown"
    ext = Path(filename).suffix.lower()
    
    if ext in ['.png', '.jpg', '.jpeg', '.gif', '.webp', '.bmp']:
        filetype = 'image'
    elif ext in ['.txt', '.md', '.json', '.yaml', '.yml', '.log', '.csv']:
        filetype = 'text'
    elif ext in ['.py', '.js', '.ts', '.sh', '.bash', '.php', '.html', '.css']:
        filetype = 'code'
    else:
        filetype = 'binary'
    
    # Sauvegarder le fichier
    filepath = os.path.join(UPLOAD_DIR, f"{file_id}{ext}")
    content = await file.read()
    
    with open(filepath, 'wb') as f:
        f.write(content)
    
    # Enregistrer en DB
    conn = get_db()
    c = conn.cursor()
    c.execute('''INSERT INTO uploads (id, filename, filepath, filetype, filesize, conversation_id)
                 VALUES (?, ?, ?, ?, ?, ?)''',
              (file_id, filename, filepath, filetype, len(content), conversation_id))
    conn.commit()
    conn.close()
    
    return {
        "id": file_id,
        "filename": filename,
        "filetype": filetype,
        "size": len(content)
    }

def get_upload_info(file_id: str) -> dict:
    """R√©cup√©rer les infos d'un fichier upload√©"""
    conn = get_db()
    c = conn.cursor()
    c.execute('SELECT * FROM uploads WHERE id = ?', (file_id,))
    row = c.fetchone()
    conn.close()
    
    if row:
        return dict(row)
    return None

def get_file_content(file_id: str) -> tuple:
    """R√©cup√©rer le contenu d'un fichier (content, filetype)"""
    info = get_upload_info(file_id)
    if not info:
        return None, None
    
    filepath = info['filepath']
    filetype = info['filetype']
    
    if filetype == 'image':
        with open(filepath, 'rb') as f:
            content = base64.b64encode(f.read()).decode('utf-8')
        return content, 'image'
    else:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            return content, 'text'
        except:
            with open(filepath, 'rb') as f:
                content = f.read().hex()
            return content, 'binary'

# ===== GESTION HISTORIQUE =====

def create_conversation(title: str = None) -> str:
    """Cr√©er une nouvelle conversation"""
    conv_id = str(uuid.uuid4())[:12]
    conn = get_db()
    c = conn.cursor()
    c.execute('INSERT INTO conversations (id, title) VALUES (?, ?)',
              (conv_id, title or f"Conversation {datetime.now().strftime('%Y-%m-%d %H:%M')}"))
    conn.commit()
    conn.close()
    return conv_id

def add_message(conversation_id: str, role: str, content: str, model_used: str = None):
    """Ajouter un message √† une conversation"""
    conn = get_db()
    c = conn.cursor()
    c.execute('''INSERT INTO messages (conversation_id, role, content, model_used)
                 VALUES (?, ?, ?, ?)''', (conversation_id, role, content, model_used))
    c.execute('UPDATE conversations SET updated_at = CURRENT_TIMESTAMP WHERE id = ?',
              (conversation_id,))
    conn.commit()
    conn.close()

def get_conversations(limit: int = 20) -> list:
    """R√©cup√©rer les conversations r√©centes"""
    conn = get_db()
    c = conn.cursor()
    c.execute('''SELECT c.*, 
                 (SELECT content FROM messages WHERE conversation_id = c.id ORDER BY created_at LIMIT 1) as first_message
                 FROM conversations c 
                 ORDER BY updated_at DESC LIMIT ?''', (limit,))
    rows = c.fetchall()
    conn.close()
    return [dict(row) for row in rows]

def get_conversation_messages(conversation_id: str) -> list:
    """R√©cup√©rer les messages d'une conversation"""
    conn = get_db()
    c = conn.cursor()
    c.execute('SELECT * FROM messages WHERE conversation_id = ? ORDER BY created_at', (conversation_id,))
    rows = c.fetchall()
    conn.close()
    return [dict(row) for row in rows]

def update_conversation_title(conversation_id: str, title: str):
    """Mettre √† jour le titre d'une conversation"""
    conn = get_db()
    c = conn.cursor()
    c.execute('UPDATE conversations SET title = ? WHERE id = ?', (title, conversation_id))
    conn.commit()
    conn.close()

def delete_conversation(conversation_id: str):
    """Supprimer une conversation et ses messages"""
    conn = get_db()
    c = conn.cursor()
    c.execute('DELETE FROM messages WHERE conversation_id = ?', (conversation_id,))
    c.execute('DELETE FROM uploads WHERE conversation_id = ?', (conversation_id,))
    c.execute('DELETE FROM conversations WHERE id = ?', (conversation_id,))
    conn.commit()
    conn.close()

# ===== EX√âCUTION DES OUTILS (AVEC PONT MCP) =====

async def execute_tool(tool_name: str, params: dict, uploaded_files: dict = None) -> str:
    """Ex√©cuter un outil et retourner le r√©sultat"""
    
    try:
        if tool_name == "execute_command":
            cmd = params.get("command", "")
            if not cmd:
                return "Erreur: commande vide"

            # --- PONT MCP ---
            if MCP_ENABLED:
                return await mcp_run_command(cmd)
            # ----------------

            # Fallback Local (si MCP d√©sactiv√©)
            if SECURITY_ENABLED:
                allowed, reason = validate_command(cmd)
                audit_log.log_command(cmd, allowed=allowed, reason=reason)
                if not allowed:
                    return f"üö´ Commande bloqu√©e: {reason}"

            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=60)
            output = result.stdout or result.stderr or "(aucune sortie)"
            return f"Commande: {cmd}\nSortie:\n{output[:3000]}"
        
        elif tool_name == "read_file":
            path = params.get("path", "")
            if not path:
                return "Erreur: chemin requis"

            # --- PONT MCP ---
            if MCP_ENABLED:
                return await mcp_read_file(path)
            # ----------------

            # Fallback Local
            if SECURITY_ENABLED:
                try:
                    path = validate_path(path, write=False)
                except PathNotAllowedError as e:
                    return f"üö´ Acc√®s refus√©: {e}"

            if not os.path.exists(path):
                return f"Erreur: fichier non trouv√©: {path}"
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read(10000)
                return f"Contenu de {path}:\n{content}"
            except Exception as e:
                return f"Erreur lecture: {e}"
        
        elif tool_name == "write_file":
            path = params.get("path", "")
            content = params.get("content", "")
            if not path:
                return "Erreur: chemin requis"

            # --- PONT MCP ---
            if MCP_ENABLED:
                return await mcp_write_file(path, content)
            # ----------------

            # Fallback Local
            if SECURITY_ENABLED:
                try:
                    path = validate_path(path, write=True)
                except PathNotAllowedError as e:
                    return f"üö´ Acc√®s refus√©: {e}"

            try:
                os.makedirs(os.path.dirname(path), exist_ok=True)
                with open(path, 'w', encoding='utf-8') as f:
                    f.write(content)
                return f"Fichier √©crit: {path}"
            except Exception as e:
                return f"Erreur √©criture: {e}"
        
        elif tool_name == "system_info":
            # On laisse le system_info local si le bridge ne le g√®re pas sp√©cifiquement,
            # ou on pourrait le router via mcp_run_command si besoin.
            # Ici on garde le local pour la simplicit√©, sauf si on veut tout router.
            # Si on veut router, on utiliserait mcp_run_command pour chaque sous-commande.
            cmds = {
                "hostname": "hostname",
                "uptime": "uptime",
                "cpu": "lscpu | head -20",
                "memory": "free -h",
                "disk": "df -h /",
            }
            results = []
            for name, cmd in cmds.items():
                if MCP_ENABLED:
                    res = await mcp_run_command(cmd)
                    results.append(f"=== {name.upper()} ===\n{res}")
                else:
                    r = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)
                    results.append(f"=== {name.upper()} ===\n{r.stdout or r.stderr}")
            return "\n".join(results)
        
        elif tool_name == "docker_status":
            cmd = "docker ps -a --format 'table {{.Names}}\t{{.Status}}\t{{.Ports}}'"
            if MCP_ENABLED:
                return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            return f"Conteneurs Docker:\n{result.stdout or result.stderr}"
        
        elif tool_name == "docker_logs":
            container = params.get("container", "")
            lines = params.get("lines", 50)
            if not container: return "Erreur: nom du conteneur requis"
            cmd = f"docker logs --tail {lines} {container} 2>&1"
            if MCP_ENABLED:
                return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            return f"Logs de {container}:\n{result.stdout or result.stderr}"
        
        elif tool_name == "docker_restart":
            container = params.get("container", "")
            if not container: return "Erreur: nom du conteneur requis"
            cmd = f"docker restart {container}"
            if MCP_ENABLED:
                return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=60)
            return f"Red√©marrage de {container}: {result.stdout or result.stderr}"
        
        elif tool_name == "disk_usage":
            path = params.get("path", "/")
            cmd = f"du -sh {path}/* 2>/dev/null | sort -rh | head -20"
            if MCP_ENABLED:
                return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            return f"Espace disque:\n{result.stdout}"
        
        elif tool_name == "service_status":
            service = params.get("service", "")
            if not service: return "Erreur: service requis"
            cmd = f"systemctl status {service} --no-pager"
            if MCP_ENABLED:
                return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)
            return f"Statut de {service}:\n{result.stdout or result.stderr}"
        
        elif tool_name == "service_control":
            service = params.get("service", "")
            action = params.get("action", "")
            if not service or action not in ["start", "stop", "restart"]:
                return "Erreur: service et action requis"
            cmd = f"sudo systemctl {action} {service}"
            if MCP_ENABLED:
                return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            return f"Action {action} sur {service}: {result.stdout or result.stderr or 'OK'}"
        
        elif tool_name == "list_directory":
            path = params.get("path", "/home/lalpha")
            cmd = f"ls -la {path}"
            if MCP_ENABLED:
                return await mcp_run_command(cmd)
            if not os.path.exists(path): return f"Erreur: r√©pertoire non trouv√©: {path}"
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)
            return f"Contenu de {path}:\n{result.stdout}"
        
        elif tool_name == "search_files":
            pattern = params.get("pattern", "*")
            path = params.get("path", "/home/lalpha")
            content_search = params.get("content", False)
            if content_search:
                cmd = f"grep -r '{pattern}' {path} 2>/dev/null | head -50"
            else:
                cmd = f"find {path} -name '{pattern}' 2>/dev/null | head -50"
            if MCP_ENABLED:
                return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            return f"Recherche '{pattern}':\n{result.stdout or 'Aucun r√©sultat'}"
        
        elif tool_name == "udm_status":
            # SSH doit √™tre g√©r√© par l'h√¥te ou le bridge
            cmd = "ssh -i /home/lalpha/.ssh/id_rsa_udm -o StrictHostKeyChecking=no -o ConnectTimeout=5 root@10.10.10.1 'uptime; echo; cat /etc/version; echo; df -h /'"
            if MCP_ENABLED:
                return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=15)
            return f"UDM-Pro Status:\n{result.stdout or result.stderr}"
        
        elif tool_name == "udm_network_info":
            cmd = "ssh -i /home/lalpha/.ssh/id_rsa_udm -o StrictHostKeyChecking=no -o ConnectTimeout=5 root@10.10.10.1 'ip addr show; echo; cat /run/dnsmasq.conf.d/*.conf 2>/dev/null | head -50'"
            if MCP_ENABLED: return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=15)
            return f"UDM-Pro Network:\n{result.stdout or result.stderr}"
        
        elif tool_name == "udm_clients":
            cmd = "ssh -i /home/lalpha/.ssh/id_rsa_udm -o StrictHostKeyChecking=no -o ConnectTimeout=5 root@10.10.10.1 'cat /proc/net/arp'"
            if MCP_ENABLED: return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=15)
            return f"Clients connect√©s:\n{result.stdout or result.stderr}"
        
        elif tool_name == "network_scan":
            cmd = "ss -tlnp | head -30"
            if MCP_ENABLED: return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)
            return f"Ports ouverts:\n{result.stdout}"
        
        elif tool_name == "ollama_list":
            # Requ√™te HTTP locale, pas besoin de MCP sauf si OLLAMA est sur une autre machine inaccessible
            async with httpx.AsyncClient() as client:
                resp = await client.get(f"{OLLAMA_URL}/api/tags", timeout=10)
                data = resp.json()
                models = [f"- {m['name']} ({m.get('size', 'N/A')})" for m in data.get('models', [])]
                return f"Mod√®les Ollama:\n" + "\n".join(models)
        
        elif tool_name == "ollama_run":
            model = params.get("model", DEFAULT_MODEL)
            prompt = params.get("prompt", "")
            if not prompt: return "Erreur: prompt requis"
            async with httpx.AsyncClient() as client:
                resp = await client.post(
                    f"{OLLAMA_URL}/api/generate",
                    json={"model": model, "prompt": prompt, "stream": False},
                    timeout=120
                )
                data = resp.json()
                return f"R√©ponse de {model}:\n{data.get('response', 'Erreur')}"
        
        elif tool_name == "analyze_image":
            image_id = params.get("image_id", "")
            question = params.get("question", "D√©cris cette image en d√©tail")
            content, ftype = get_file_content(image_id)
            if not content or ftype != 'image': return f"Erreur: image non trouv√©e ou invalide: {image_id}"
            async with httpx.AsyncClient() as client:
                resp = await client.post(
                    f"{OLLAMA_URL}/api/generate",
                    json={
                        "model": MODELS["llama-vision"]["model"],
                        "prompt": question,
                        "images": [content],
                        "stream": False
                    },
                    timeout=300
                )
                data = resp.json()
                return f"Analyse de l'image:\n{data.get('response', 'Erreur analyse')}"
        
        elif tool_name == "analyze_file":
            file_id = params.get("file_id", "")
            content, ftype = get_file_content(file_id)
            if not content: return f"Erreur: fichier non trouv√©: {file_id}"
            info = get_upload_info(file_id)
            if ftype == 'image':
                return f"C'est une image ({info['filename']}). Utilise analyze_image() pour l'analyser."
            else:
                preview = content[:5000] if len(content) > 5000 else content
                return f"Fichier: {info['filename']}\nType: {ftype}\nTaille: {info['filesize']} bytes\n\nContenu:\n{preview}"
        
        elif tool_name == "create_script":
            path = params.get("path", "")
            content = params.get("content", "")
            if not path or not content: return "Erreur: path et content requis"
            if MCP_ENABLED:
                # On utilise write_file via MCP puis chmod via run_command
                await mcp_write_file(path, content)
                await mcp_run_command(f"chmod +x {path}")
                return f"Script cr√©√© via MCP: {path}"
            try:
                os.makedirs(os.path.dirname(path), exist_ok=True)
                with open(path, 'w') as f: f.write(content)
                os.chmod(path, 0o755)
                return f"Script cr√©√©: {path} (ex√©cutable)"
            except Exception as e: return f"Erreur cr√©ation script: {e}"
        
        elif tool_name == "git_status":
            path = params.get("path", "/home/lalpha/projets")
            cmd = f"cd {path} && git status && git log --oneline -5"
            if MCP_ENABLED: return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)
            return f"Git status de {path}:\n{result.stdout or result.stderr}"

        elif tool_name == "git_diff":
            path = params.get("path", "/home/lalpha/projets")
            cmd = f"cd {path} && git diff --stat && echo '\n=== D√©tails ===' && git diff"
            if MCP_ENABLED: return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            output = result.stdout or result.stderr
            if not output.strip(): return f"Aucune modification dans {path}"
            return f"Git diff de {path}:\n{output[:3000]}"
        
        elif tool_name == "git_log":
            path = params.get("path", "/home/lalpha/projets")
            n = params.get("n", 10)
            cmd = f"cd {path} && git log --oneline --graph -n {n}"
            if MCP_ENABLED: return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)
            return f"Git log de {path} ({n} derniers commits):\n{result.stdout or result.stderr}"
        
        elif tool_name == "git_commit":
            path = params.get("path", "/home/lalpha/projets")
            message = params.get("message", "")
            add_all = params.get("add_all", True)
            if not message: return "Erreur: message de commit requis"
            add_cmd = "git add -A && " if add_all else ""
            cmd = f'cd {path} && {add_cmd}git commit -m "{message}"'
            if MCP_ENABLED: return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            return f"Git commit dans {path}:\n{result.stdout or result.stderr}"
        
        elif tool_name == "git_branch":
            path = params.get("path", "/home/lalpha/projets")
            branch = params.get("branch", "")
            if branch:
                cmd = f"cd {path} && git checkout {branch}"
            else:
                cmd = f"cd {path} && git branch -a"
            if MCP_ENABLED: return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)
            return f"Git branch: {result.stdout or result.stderr}"
        
        elif tool_name == "git_pull":
            path = params.get("path", "/home/lalpha/projets")
            cmd = f"cd {path} && git pull"
            if MCP_ENABLED: return await mcp_run_command(cmd)
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=60)
            return f"Git pull dans {path}:\n{result.stdout or result.stderr}"

        # Les outils m√©moire utilisent ChromaDB directement (backend), pas besoin de passer par MCP
        # sauf si ChromaDB √©tait sur une autre machine inaccessible
        elif tool_name == "memory_store":
            key = params.get("key", "")
            value = params.get("value", "")
            if not key: return "Erreur: cl√© requise"
            try:
                chroma_client = chromadb.HttpClient(host="chromadb", port=8000, settings=Settings(anonymized_telemetry=False))
                collection = chroma_client.get_or_create_collection(name="ai_orchestrator_memory", metadata={"description": "M√©moire s√©mantique de l'AI Orchestrator"})
                doc_content = f"{key}: {value}"
                collection.upsert(documents=[doc_content], ids=[f"mem_{key}"], metadatas=[{"key": key, "type": "user", "timestamp": datetime.now().isoformat()}])
                conn = get_db()
                c = conn.cursor()
                c.execute('INSERT OR REPLACE INTO memory (key, value, updated_at) VALUES (?, ?, CURRENT_TIMESTAMP)', (key, value))
                conn.commit()
                conn.close()
                return f"‚úÖ M√©moris√© dans m√©moire s√©mantique: {key} = {value[:100]}..."
            except Exception as e: return f"Erreur m√©moire: {str(e)}"
        
        elif tool_name == "memory_recall":
            query = params.get("query", params.get("key", "all"))
            try:
                chroma_client = chromadb.HttpClient(host="chromadb", port=8000, settings=Settings(anonymized_telemetry=False))
                collection = chroma_client.get_or_create_collection(name="ai_orchestrator_memory")
                if query.lower() == "all":
                    results = collection.get(limit=20, include=["documents", "metadatas"])
                    if results and results.get("documents"):
                        memories = []
                        for i, doc in enumerate(results["documents"]):
                            meta = results["metadatas"][i] if results.get("metadatas") else {}
                            mem_type = meta.get("type", "unknown")
                            memories.append(f"üß† [{mem_type}] {doc}")
                        return "üìö M√©moires disponibles:\n" + "\n".join(memories)
                    return "M√©moire vide"
                else:
                    results = collection.query(query_texts=[query], n_results=5, include=["documents", "metadatas", "distances"])
                    if results and results.get("documents") and results["documents"][0]:
                        memories = []
                        for i, doc in enumerate(results["documents"][0]):
                            distance = results["distances"][0][i] if results.get("distances") else 0
                            similarity = max(0, 100 - (distance * 50))
                            memories.append(f"üß† [{similarity:.1f}%] {doc}")
                        return f"üîç Recherche '{query}':\n" + "\n".join(memories)
                    return f"Aucune m√©moire trouv√©e pour: {query}"
            except Exception as e:
                conn = get_db()
                c = conn.cursor()
                c.execute('SELECT key, value FROM memory ORDER BY updated_at DESC LIMIT 20')
                rows = c.fetchall()
                conn.close()
                if rows: return "M√©moire (SQLite fallback):\n" + "\n".join([f"- {r[0]}: {r[1][:100]}" for r in rows])
                return f"Erreur ChromaDB: {str(e)}"
        
        elif tool_name == "web_request":
            url = params.get("url", "")
            method = params.get("method", "GET").upper()
            data = params.get("data", None)
            if not url: return "Erreur: URL requise"
            async with httpx.AsyncClient() as client:
                if method == "POST": resp = await client.post(url, json=json.loads(data) if data else None, timeout=30)
                else: resp = await client.get(url, timeout=30)
                return f"HTTP {resp.status_code}:\n{resp.text[:2000]}"
        
        elif tool_name == "memory_stats":
            if AUTO_LEARN_ENABLED:
                stats = get_memory_stats()
                result = f"üìä Statistiques m√©moire:\n"
                result += f"   Total: {stats.get('total', 0)} souvenirs\n"
                result += f"   Par type: {stats.get('by_type', {})}\n"
                result += f"   Par cat√©gorie: {stats.get('by_category', {})}"
                return result
            return "Auto-apprentissage non activ√©"
        
        elif tool_name == "final_answer":
            return params.get("answer", "")
        
        else:
            return f"Outil inconnu: {tool_name}"
    
    except subprocess.TimeoutExpired:
        return f"Timeout: l'outil {tool_name} a pris trop de temps"
    except Exception as e:
        return f"Erreur lors de l'ex√©cution de {tool_name}: {str(e)}"

# ===== PARSING DES ACTIONS =====

def parse_action(text: str) -> tuple:
    """Parser une action du format: tool_name(param="value")"""
    text = text.strip()
    
    # CAS SPECIAL: final_answer
    if "final_answer" in text:
        match = re.search(r'final_answer\s*\(\s*answer\s*=\s*"(.*)"?\s*\)?$', text, re.DOTALL)
        if match:
            answer = match.group(1).rstrip()
            while answer and answer[-1] in '")\'\\': answer = answer[:-1]
            return "final_answer", {"answer": answer.rstrip()}
        
        idx = text.find('answer="')
        if idx >= 0:
            content = text[idx + 8:].rstrip()
            if content.endswith('")'): content = content[:-2]
            elif content.endswith('"'): content = content[:-1]
            elif content.endswith(')'): content = content[:-1]
            return "final_answer", {"answer": content.strip()}
        
        idx = text.find("answer='")
        if idx >= 0:
            content = text[idx + 8:].rstrip()
            if content.endswith("')"): content = content[:-2]
            elif content.endswith("'"): content = content[:-1]
            return "final_answer", {"answer": content.strip()}
    
    match = re.search(r'(\w+)\s*\(([^)]*)\)', text)
    if not match: return None, {}
    
    tool_name = match.group(1)
    params_str = match.group(2)
    
    params = {}
    if params_str.strip():
        for m in re.finditer(r'(\w+)\s*=\s*"([^"]*)"', params_str): params[m.group(1)] = m.group(2)
        for m in re.finditer(r"(\w+)\s*=\s*'([^']*)'", params_str): params[m.group(1)] = m.group(2)
    
    return tool_name, params

# ===== BOUCLE REACT =====

async def react_loop(
    user_message: str,
    model: str,
    conversation_id: str,
    uploaded_files: list = None,
    websocket: WebSocket = None
):
    """Boucle ReAct principale"""
    
    # Construire le contexte des fichiers upload√©s
    files_context = ""
    if uploaded_files:
        files_context = "\n\nFichiers attach√©s par l'utilisateur:\n"
        for f in uploaded_files:
            files_context += f"- ID: {f['id']} | Nom: {f['filename']} | Type: {f['filetype']}\n"
        files_context += "\nUtilise analyze_file(file_id=\"...\") ou analyze_image(image_id=\"...\") pour les examiner.\n"
    
    # Prompt syst√®me
    tools_desc = "\n".join([
        f"- {name}: {info['description']}\n  Exemple: {info['example']}"
        for name, info in TOOLS.items()
    ])
    
    system_prompt = f"""Tu es un EXPERT DevOps/SysAdmin/D√©veloppeur pour le serveur 4LB.ca.

## üñ•Ô∏è INFRASTRUCTURE 4LB.ca
- **Serveur**: Ubuntu 25.10, AMD Ryzen 9 7900X (12 cores), RTX 5070 Ti 16GB, 64GB RAM
- **Projets**: /home/lalpha/projets/ (ai-tools/, clients/, infrastructure/)
- **Clients**: /home/lalpha/projets/clients/jsr/ (JSR, JSR-solutions)
- **Docker**: unified-stack (14 services sur unified-net)
- **Domaines**: ai.4lb.ca, llm.4lb.ca, grafana.4lb.ca, jsr.4lb.ca
- **LLM**: Ollama avec qwen2.5-coder:32b, deepseek-coder:33b, qwen3-vl:32b

## üéØ M√âTHODOLOGIE OBLIGATOIRE

### Pour ANALYSER un projet:
1. **PLAN**: D√©cris en 3 lignes ce que tu vas faire
2. **STRUCTURE**: list_directory() pour voir l'arborescence
3. **FICHIERS CL√âS**: read_file() sur package.json, README.md, Dockerfile, fichiers source principaux
4. **SYNTH√àSE**: Compte-rendu STRUCTUR√â avec le format ci-dessous

### FORMAT DE COMPTE-RENDU:
