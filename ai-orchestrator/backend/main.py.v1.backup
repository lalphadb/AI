#!/usr/bin/env python3
"""
Orchestrateur IA 4LB.ca - Backend FastAPI
Agent autonome avec boucle ReAct (Think → Act → Observe → Repeat)
"""

import os
import json
import asyncio
import sqlite3
import httpx
import subprocess
import re
from datetime import datetime
from typing import Optional, List, Dict, Any, AsyncGenerator
from contextlib import asynccontextmanager

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# Configuration
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://10.10.10.46:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen2.5-coder:32b-instruct-q4_K_M")
DB_PATH = "/data/orchestrator.db"
MAX_ITERATIONS = 10

# ===== DÉFINITION DES OUTILS =====

TOOLS = {
    "execute_command": {
        "description": "Exécuter une commande bash sur le serveur Ubuntu",
        "parameters": {"command": "string - La commande à exécuter"},
        "example": "execute_command(command='ls -la /home/lalpha')"
    },
    "system_info": {
        "description": "Obtenir les informations système (CPU, RAM, disque)",
        "parameters": {},
        "example": "system_info()"
    },
    "docker_status": {
        "description": "Voir l'état de tous les conteneurs Docker",
        "parameters": {},
        "example": "docker_status()"
    },
    "docker_logs": {
        "description": "Voir les logs d'un conteneur Docker",
        "parameters": {"container": "string - Nom du conteneur", "lines": "int - Nombre de lignes (défaut: 50)"},
        "example": "docker_logs(container='traefik', lines=100)"
    },
    "disk_usage": {
        "description": "Analyser l'utilisation du disque",
        "parameters": {"path": "string - Chemin à analyser (défaut: /)"},
        "example": "disk_usage(path='/home/lalpha')"
    },
    "service_status": {
        "description": "Vérifier le statut d'un service systemd",
        "parameters": {"service": "string - Nom du service"},
        "example": "service_status(service='ollama')"
    },
    "service_control": {
        "description": "Contrôler un service (start, stop, restart)",
        "parameters": {"service": "string - Nom du service", "action": "string - Action: start, stop, restart"},
        "example": "service_control(service='ollama', action='restart')"
    },
    "read_file": {
        "description": "Lire le contenu d'un fichier",
        "parameters": {"path": "string - Chemin du fichier"},
        "example": "read_file(path='/home/lalpha/projets/README.md')"
    },
    "write_file": {
        "description": "Écrire du contenu dans un fichier",
        "parameters": {"path": "string - Chemin du fichier", "content": "string - Contenu à écrire"},
        "example": "write_file(path='/home/lalpha/test.txt', content='Hello')"
    },
    "list_directory": {
        "description": "Lister le contenu d'un répertoire",
        "parameters": {"path": "string - Chemin du répertoire"},
        "example": "list_directory(path='/home/lalpha/projets')"
    },
    "udm_status": {
        "description": "Obtenir le statut du UDM-Pro UniFi",
        "parameters": {},
        "example": "udm_status()"
    },
    "udm_network_info": {
        "description": "Informations réseau du UDM-Pro (VLANs, clients)",
        "parameters": {},
        "example": "udm_network_info()"
    },
    "network_scan": {
        "description": "Scanner les ports ouverts du serveur",
        "parameters": {},
        "example": "network_scan()"
    },
    "ollama_list": {
        "description": "Lister les modèles Ollama installés",
        "parameters": {},
        "example": "ollama_list()"
    },
    "ollama_run": {
        "description": "Exécuter une requête sur un modèle Ollama",
        "parameters": {"model": "string - Nom du modèle", "prompt": "string - Prompt"},
        "example": "ollama_run(model='qwen2.5-coder:32b', prompt='Hello')"
    },
    "create_script": {
        "description": "Créer un script bash exécutable",
        "parameters": {"path": "string - Chemin", "content": "string - Contenu"},
        "example": "create_script(path='/home/lalpha/scripts/test.sh', content='#!/bin/bash\\necho OK')"
    },
    "memory_store": {
        "description": "Stocker une information en mémoire",
        "parameters": {"key": "string - Clé", "value": "string - Valeur"},
        "example": "memory_store(key='projet', value='Migration')"
    },
    "memory_recall": {
        "description": "Rappeler une information de la mémoire",
        "parameters": {"key": "string - Clé (ou 'all')"},
        "example": "memory_recall(key='projet')"
    },
    "final_answer": {
        "description": "Fournir la réponse finale",
        "parameters": {"answer": "string - La réponse structurée"},
        "example": "final_answer(answer='Voici le résultat...')"
    }
}

# ===== EXÉCUTION DES OUTILS =====

async def run_command(cmd: str, timeout: int = 60) -> str:
    """Exécuter une commande bash"""
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)
        output = result.stdout or result.stderr
        return output[:5000] if len(output) > 5000 else output
    except subprocess.TimeoutExpired:
        return f"ERREUR: Timeout après {timeout}s"
    except Exception as e:
        return f"ERREUR: {str(e)}"


async def execute_tool(tool_name: str, params: Dict[str, Any], db: sqlite3.Connection) -> str:
    """Exécuter un outil et retourner le résultat"""
    try:
        if tool_name == "execute_command":
            return await run_command(params.get("command", "echo 'No command'"))
        
        elif tool_name == "system_info":
            cmd = """echo "=== CPU ===" && lscpu | head -20 && echo -e "\n=== MÉMOIRE ===" && free -h && echo -e "\n=== DISQUE ===" && df -h / /mnt/ollama-models 2>/dev/null"""
            return await run_command(cmd)
        
        elif tool_name == "docker_status":
            return await run_command("docker ps -a --format 'table {{.Names}}\t{{.Status}}\t{{.Ports}}' 2>/dev/null || echo 'Docker non disponible'")
        
        elif tool_name == "docker_logs":
            container = params.get("container", "traefik")
            lines = params.get("lines", 50)
            return await run_command(f"docker logs --tail {lines} {container} 2>&1")
        
        elif tool_name == "disk_usage":
            path = params.get("path", "/")
            return await run_command(f"du -sh {path}/* 2>/dev/null | sort -rh | head -20")
        
        elif tool_name == "service_status":
            service = params.get("service", "ollama")
            return await run_command(f"systemctl status {service} --no-pager 2>&1 | head -30")
        
        elif tool_name == "service_control":
            service = params.get("service", "")
            action = params.get("action", "status")
            if action in ["start", "stop", "restart", "enable", "disable"]:
                return await run_command(f"sudo systemctl {action} {service} && echo 'OK: {action} {service}'")
            return "ERREUR: Action non valide"
        
        elif tool_name == "read_file":
            path = params.get("path", "")
            return await run_command(f"cat '{path}' 2>&1 | head -200")
        
        elif tool_name == "write_file":
            path = params.get("path", "")
            content = params.get("content", "")
            escaped = content.replace("'", "'\"'\"'")
            return await run_command(f"echo '{escaped}' > '{path}' && echo 'Fichier écrit: {path}'")
        
        elif tool_name == "list_directory":
            path = params.get("path", "/home/lalpha")
            return await run_command(f"ls -la '{path}' 2>&1")
        
        elif tool_name == "udm_status":
            return await run_command("ssh -i /home/lalpha/.ssh/id_rsa_udm -o ConnectTimeout=5 root@10.10.10.1 'uptime && cat /etc/unifi-os/release' 2>&1 || echo 'UDM-Pro inaccessible'")
        
        elif tool_name == "udm_network_info":
            return await run_command("ssh -i /home/lalpha/.ssh/id_rsa_udm -o ConnectTimeout=5 root@10.10.10.1 'ip addr show | head -50' 2>&1 || echo 'UDM-Pro inaccessible'")
        
        elif tool_name == "network_scan":
            return await run_command("ss -tlnp 2>/dev/null | head -30")
        
        elif tool_name == "ollama_list":
            return await run_command("curl -s http://localhost:11434/api/tags 2>&1 | python3 -c \"import sys,json; d=json.load(sys.stdin); print(chr(10).join([f\\\"{m['name']} ({m['details'].get('parameter_size','?')})\\\" for m in d.get('models',[])])[:2000])\" 2>&1 || echo 'Ollama non disponible'")
        
        elif tool_name == "ollama_run":
            model = params.get("model", OLLAMA_MODEL)
            prompt = params.get("prompt", "Hello")[:500]
            escaped = json.dumps({"model": model, "prompt": prompt, "stream": False})
            return await run_command(f"curl -s http://localhost:11434/api/generate -d '{escaped}' | python3 -c \"import sys,json; print(json.load(sys.stdin).get('response',''))[:1000]\"", timeout=120)
        
        elif tool_name == "create_script":
            path = params.get("path", "/tmp/script.sh")
            content = params.get("content", "#!/bin/bash\necho OK")
            escaped = content.replace("'", "'\"'\"'")
            return await run_command(f"echo '{escaped}' > '{path}' && chmod +x '{path}' && echo 'Script créé: {path}'")
        
        elif tool_name == "memory_store":
            key = params.get("key", "")
            value = params.get("value", "")
            cursor = db.cursor()
            cursor.execute("INSERT OR REPLACE INTO memory (key, value, updated_at) VALUES (?, ?, ?)", (key, value, datetime.now().isoformat()))
            db.commit()
            return f"Mémoire stockée: {key} = {value[:100]}..."
        
        elif tool_name == "memory_recall":
            key = params.get("key", "all")
            cursor = db.cursor()
            if key == "all":
                cursor.execute("SELECT key, value FROM memory ORDER BY updated_at DESC LIMIT 20")
                rows = cursor.fetchall()
                return "\n".join([f"{k}: {v}" for k, v in rows]) or "Mémoire vide"
            else:
                cursor.execute("SELECT value FROM memory WHERE key = ?", (key,))
                row = cursor.fetchone()
                return row[0] if row else f"Clé '{key}' non trouvée"
        
        elif tool_name == "final_answer":
            return params.get("answer", "Réponse non spécifiée")
        
        else:
            return f"ERREUR: Outil '{tool_name}' non reconnu"
    
    except Exception as e:
        return f"ERREUR lors de l'exécution de {tool_name}: {str(e)}"

# ===== BOUCLE ReAct =====

def build_system_prompt() -> str:
    """Construire le prompt système"""
    tools_desc = "\n".join([f"- **{name}**: {info['description']}\n  Exemple: {info['example']}" for name, info in TOOLS.items()])
    
    return f"""Tu es un agent IA autonome puissant qui gère l'infrastructure du serveur lalpha-server-1.

## OUTILS DISPONIBLES
{tools_desc}

## MÉTHODE DE TRAVAIL (ReAct)
À chaque étape:
1. THINK: Réfléchir
2. ACT: Appeler UN outil
3. OBSERVE: Analyser le résultat
4. REPEAT: Continuer jusqu'à final_answer

## FORMAT OBLIGATOIRE

THINK: [Ta réflexion]
ACTION: tool_name(param1="value1", param2="value2")

OU pour terminer:

THINK: [Résumé]
ACTION: final_answer(answer="Ta réponse complète")

## RÈGLES
1. UNE SEULE action par réponse
2. Syntaxe exacte: tool_name(param="value")
3. Strings entre guillemets
4. final_answer() pour terminer
5. Max 10 itérations

## CONTEXTE
- Serveur: lalpha-server-1 (Ubuntu 25.10)
- CPU: Ryzen 9 7900X (12c/24t)
- GPU: RTX 5070 Ti (16GB)
- RAM: 64GB
- IP: 10.10.10.46
- Ollama: localhost:11434"""


def parse_action(response: str) -> tuple:
    """Parser l'action du LLM"""
    action_match = re.search(r'ACTION:\s*(\w+)\s*\((.*?)\)\s*$', response, re.MULTILINE | re.DOTALL)
    if not action_match:
        return None, {}
    
    tool_name = action_match.group(1)
    params_str = action_match.group(2).strip()
    params = {}
    if params_str:
        for match in re.finditer(r'(\w+)\s*=\s*["\'](.+?)["\']', params_str, re.DOTALL):
            params[match.group(1)] = match.group(2)
    return tool_name, params


async def call_ollama(prompt: str, system: str) -> str:
    """Appeler Ollama"""
    async with httpx.AsyncClient(timeout=180.0) as client:
        try:
            response = await client.post(
                f"{OLLAMA_URL}/api/generate",
                json={"model": OLLAMA_MODEL, "prompt": prompt, "system": system, "stream": False,
                      "options": {"temperature": 0.2, "top_p": 0.9, "num_ctx": 8192}}
            )
            return response.json().get("response", "")
        except Exception as e:
            return f"ERREUR Ollama: {str(e)}"


async def react_loop(user_message: str, db: sqlite3.Connection, send_update) -> str:
    """Boucle ReAct principale"""
    system_prompt = build_system_prompt()
    conversation = f"UTILISATEUR: {user_message}\n\n"
    final_answer = None
    
    for iteration in range(MAX_ITERATIONS):
        await send_update({"type": "thinking", "iteration": iteration + 1, "message": f"Réflexion... ({iteration + 1}/{MAX_ITERATIONS})"})
        
        llm_response = await call_ollama(conversation, system_prompt)
        
        think_match = re.search(r'THINK:\s*(.+?)(?=ACTION:|$)', llm_response, re.DOTALL)
        think_text = think_match.group(1).strip() if think_match else ""
        
        await send_update({"type": "think", "iteration": iteration + 1, "content": think_text})
        
        tool_name, params = parse_action(llm_response)
        
        if not tool_name:
            conversation += f"ASSISTANT:\n{llm_response}\n\nSYSTEM: Format invalide. Utilise ACTION: tool_name(params)\n\n"
            continue
        
        await send_update({"type": "action", "iteration": iteration + 1, "tool": tool_name, "params": params})
        
        if tool_name == "final_answer":
            final_answer = params.get("answer", "Réponse non spécifiée")
            break
        
        result = await execute_tool(tool_name, params, db)
        await send_update({"type": "observation", "iteration": iteration + 1, "tool": tool_name, "result": result[:1000]})
        
        conversation += f"ASSISTANT:\nTHINK: {think_text}\nACTION: {tool_name}({json.dumps(params)})\n\nOBSERVATION: {result}\n\n"
    
    return final_answer or "Max itérations atteintes."

# ===== BASE DE DONNÉES =====

def init_db():
    """Initialiser SQLite"""
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    conn = sqlite3.connect(DB_PATH, check_same_thread=False)
    cursor = conn.cursor()
    cursor.execute("""CREATE TABLE IF NOT EXISTS conversations (id INTEGER PRIMARY KEY AUTOINCREMENT, user_message TEXT, assistant_response TEXT, created_at TEXT)""")
    cursor.execute("""CREATE TABLE IF NOT EXISTS memory (key TEXT PRIMARY KEY, value TEXT, updated_at TEXT)""")
    conn.commit()
    return conn


# ===== APPLICATION FastAPI =====

@asynccontextmanager
async def lifespan(app: FastAPI):
    app.state.db = init_db()
    yield
    app.state.db.close()


app = FastAPI(title="Orchestrateur IA 4LB", description="Agent IA autonome ReAct", version="1.0.0", lifespan=lifespan)

app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])


class ChatRequest(BaseModel):
    message: str


@app.get("/")
async def root():
    return {"status": "ok", "service": "Orchestrateur IA 4LB", "version": "1.0.0"}


@app.get("/health")
async def health():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}


@app.get("/tools")
async def get_tools():
    return {"tools": TOOLS}


@app.websocket("/ws/chat")
async def websocket_chat(websocket: WebSocket):
    await websocket.accept()
    db = app.state.db
    
    try:
        while True:
            data = await websocket.receive_json()
            message = data.get("message", "")
            if not message:
                await websocket.send_json({"type": "error", "message": "Message vide"})
                continue
            
            async def send_update(update):
                await websocket.send_json(update)
            
            await send_update({"type": "start", "message": message})
            final_response = await react_loop(message, db, send_update)
            
            cursor = db.cursor()
            cursor.execute("INSERT INTO conversations (user_message, assistant_response, created_at) VALUES (?, ?, ?)", (message, final_response, datetime.now().isoformat()))
            db.commit()
            
            await send_update({"type": "complete", "response": final_response})
    
    except WebSocketDisconnect:
        pass
    except Exception as e:
        await websocket.send_json({"type": "error", "message": str(e)})


@app.post("/api/chat")
async def api_chat(request: ChatRequest):
    db = app.state.db
    updates = []
    async def collect(u): updates.append(u)
    final = await react_loop(request.message, db, collect)
    cursor = db.cursor()
    cursor.execute("INSERT INTO conversations (user_message, assistant_response, created_at) VALUES (?, ?, ?)", (request.message, final, datetime.now().isoformat()))
    db.commit()
    return {"response": final, "iterations": len([u for u in updates if u.get("type") == "think"])}


@app.get("/api/history")
async def get_history(limit: int = 20):
    cursor = app.state.db.cursor()
    cursor.execute("SELECT id, user_message, assistant_response, created_at FROM conversations ORDER BY id DESC LIMIT ?", (limit,))
    return {"conversations": [{"id": r[0], "user": r[1], "assistant": r[2], "created_at": r[3]} for r in cursor.fetchall()]}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
