"""
ðŸ§  Module Self-Improve - Auto-amÃ©lioration avec IA

Outils disponibles:
- self_improve_analyze_logs: Analyse des logs avec Ollama
- self_improve_anomalies: DÃ©tection d'anomalies
- self_improve_suggestions: Suggestions d'optimisation
"""

import os
import subprocess
import json
import re
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List
import logging
import requests

from config.settings import (
    OLLAMA_HOST, OLLAMA_MODEL, LOGS_DIR,
    INFRA_DIR, ANOMALY_THRESHOLD
)

logger = logging.getLogger(__name__)


class SelfImproveTools:
    """Outils d'auto-amÃ©lioration avec IA locale"""
    
    def __init__(self):
        self.ollama_host = OLLAMA_HOST
        self.model = OLLAMA_MODEL
        self.analysis_history: List[Dict] = []
    
    def _call_ollama(self, prompt: str, system: Optional[str] = None) -> Dict[str, Any]:
        """Appeler l'API Ollama"""
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False
            }
            
            if system:
                payload["system"] = system
            
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json=payload,
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "response": result.get("response", ""),
                    "model": self.model,
                    "eval_count": result.get("eval_count", 0)
                }
            else:
                return {
                    "success": False,
                    "error": f"Ollama HTTP {response.status_code}: {response.text}"
                }
                
        except requests.exceptions.ConnectionError:
            return {
                "success": False,
                "error": f"Impossible de se connecter Ã  Ollama ({self.ollama_host})"
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _get_recent_logs(self, source: str, lines: int = 200) -> str:
        """RÃ©cupÃ©rer les logs rÃ©cents"""
        logs = ""
        
        if source == "docker":
            # Logs Docker
            result = subprocess.run(
                ["docker", "logs", "--tail", str(lines), "--timestamps", "traefik"],
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode == 0:
                logs += f"=== TRAEFIK LOGS ===\n{result.stdout}\n"
            
            # Logs des autres conteneurs importants
            for container in ["open-webui", "prometheus", "postgres"]:
                result = subprocess.run(
                    ["docker", "logs", "--tail", "50", "--timestamps", container],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                if result.returncode == 0:
                    logs += f"=== {container.upper()} LOGS ===\n{result.stdout}\n"
        
        elif source == "system":
            # Logs systÃ¨me via journalctl
            result = subprocess.run(
                ["journalctl", "--since", "1 hour ago", "-n", str(lines), "--no-pager"],
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode == 0:
                logs = result.stdout
        
        elif source == "ollama":
            # Logs Ollama
            result = subprocess.run(
                ["journalctl", "-u", "ollama", "-n", str(lines), "--no-pager"],
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode == 0:
                logs = result.stdout
        
        return logs[:15000]  # Limiter la taille pour Ollama
    
    def _get_system_metrics(self) -> Dict[str, Any]:
        """RÃ©cupÃ©rer les mÃ©triques systÃ¨me"""
        metrics = {}
        
        # CPU
        result = subprocess.run(
            ["grep", "-c", "^processor", "/proc/cpuinfo"],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            metrics["cpu_cores"] = int(result.stdout.strip())
        
        # Load average
        result = subprocess.run(["cat", "/proc/loadavg"], capture_output=True, text=True)
        if result.returncode == 0:
            metrics["load_avg"] = result.stdout.strip().split()[:3]
        
        # MÃ©moire
        result = subprocess.run(["free", "-m"], capture_output=True, text=True)
        if result.returncode == 0:
            lines = result.stdout.strip().split("\n")
            if len(lines) > 1:
                mem_parts = lines[1].split()
                metrics["memory"] = {
                    "total_mb": int(mem_parts[1]),
                    "used_mb": int(mem_parts[2]),
                    "free_mb": int(mem_parts[3]),
                    "usage_percent": round(int(mem_parts[2]) / int(mem_parts[1]) * 100, 1)
                }
        
        # Disque
        result = subprocess.run(["df", "-h", "/"], capture_output=True, text=True)
        if result.returncode == 0:
            lines = result.stdout.strip().split("\n")
            if len(lines) > 1:
                disk_parts = lines[1].split()
                metrics["disk"] = {
                    "total": disk_parts[1],
                    "used": disk_parts[2],
                    "available": disk_parts[3],
                    "usage_percent": disk_parts[4]
                }
        
        # GPU (si disponible)
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=utilization.gpu,memory.used,memory.total", 
             "--format=csv,noheader,nounits"],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            gpu_parts = result.stdout.strip().split(",")
            metrics["gpu"] = {
                "utilization_percent": int(gpu_parts[0].strip()),
                "memory_used_mb": int(gpu_parts[1].strip()),
                "memory_total_mb": int(gpu_parts[2].strip())
            }
        
        # Docker containers
        result = subprocess.run(
            ["docker", "ps", "--format", "{{.Names}}: {{.Status}}"],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            metrics["docker_containers"] = result.stdout.strip().split("\n")
        
        return metrics
    
    # === 1. ANALYZE LOGS ===
    def self_improve_analyze_logs(
        self, 
        source: str = "docker",
        focus: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Analyser les logs avec Ollama
        
        Args:
            source: Source des logs (docker, system, ollama)
            focus: Point d'attention spÃ©cifique
            
        Returns:
            Dict avec l'analyse et les recommandations
        """
        try:
            # RÃ©cupÃ©rer les logs
            logs = self._get_recent_logs(source)
            
            if not logs.strip():
                return {
                    "success": False,
                    "error": f"Aucun log disponible pour {source}"
                }
            
            # Construire le prompt
            system_prompt = """Tu es un expert DevOps qui analyse les logs systÃ¨me.
Tu dois:
1. Identifier les erreurs et warnings
2. DÃ©tecter les patterns anormaux
3. Proposer des actions correctives
4. Ã‰valuer la santÃ© globale (score 0-100)

RÃ©ponds en JSON structurÃ© avec: 
{
  "health_score": int,
  "errors": ["liste des erreurs"],
  "warnings": ["liste des warnings"],
  "anomalies": ["patterns anormaux"],
  "recommendations": ["actions Ã  prendre"],
  "summary": "rÃ©sumÃ© en une phrase"
}"""
            
            focus_text = f"\n\nFocus particulier sur: {focus}" if focus else ""
            
            prompt = f"""Analyse les logs suivants:{focus_text}

{logs}

Donne ton analyse en JSON."""
            
            # Appeler Ollama
            result = self._call_ollama(prompt, system_prompt)
            
            if not result["success"]:
                return result
            
            # Parser la rÃ©ponse JSON
            response_text = result["response"]
            
            # Extraire le JSON de la rÃ©ponse
            try:
                # Chercher le JSON dans la rÃ©ponse
                json_match = re.search(r'\{[\s\S]*\}', response_text)
                if json_match:
                    analysis = json.loads(json_match.group())
                else:
                    analysis = {"raw_response": response_text}
            except json.JSONDecodeError:
                analysis = {"raw_response": response_text}
            
            # Stocker dans l'historique
            self.analysis_history.append({
                "timestamp": datetime.now().isoformat(),
                "source": source,
                "analysis": analysis
            })
            
            # Sauvegarder le rapport
            report_path = LOGS_DIR / "analysis" / f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            report_path.parent.mkdir(parents=True, exist_ok=True)
            report_path.write_text(json.dumps(analysis, indent=2, ensure_ascii=False))
            
            logger.info(f"Analyse logs {source}: score={analysis.get('health_score', 'N/A')}")
            
            return {
                "success": True,
                "source": source,
                "analysis": analysis,
                "model_used": self.model,
                "report_path": str(report_path)
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    # === 2. DETECT ANOMALIES ===
    def self_improve_anomalies(self) -> Dict[str, Any]:
        """
        DÃ©tecter les anomalies systÃ¨me avec mÃ©triques
        
        Returns:
            Dict avec les anomalies dÃ©tectÃ©es et recommandations
        """
        try:
            # RÃ©cupÃ©rer les mÃ©triques
            metrics = self._get_system_metrics()
            
            # Construire le prompt
            system_prompt = """Tu es un expert en monitoring et dÃ©tection d'anomalies.
Analyse les mÃ©triques systÃ¨me et dÃ©tecte les problÃ¨mes potentiels.

RÃ©ponds en JSON:
{
  "anomalies": [
    {"type": "cpu|memory|disk|gpu|docker", "severity": "low|medium|high|critical", "description": "..."}
  ],
  "alerts": ["alertes immÃ©diates"],
  "optimizations": ["suggestions d'optimisation"],
  "overall_status": "healthy|warning|critical"
}"""
            
            prompt = f"""Analyse ces mÃ©triques systÃ¨me:

{json.dumps(metrics, indent=2)}

DÃ©tecte les anomalies et problÃ¨mes potentiels."""
            
            # Appeler Ollama
            result = self._call_ollama(prompt, system_prompt)
            
            if not result["success"]:
                # Fallback: dÃ©tection basique sans IA
                anomalies = self._basic_anomaly_detection(metrics)
                return {
                    "success": True,
                    "metrics": metrics,
                    "anomalies": anomalies,
                    "method": "basic",
                    "warning": "Ollama non disponible, analyse basique utilisÃ©e"
                }
            
            # Parser la rÃ©ponse
            response_text = result["response"]
            
            try:
                json_match = re.search(r'\{[\s\S]*\}', response_text)
                if json_match:
                    analysis = json.loads(json_match.group())
                else:
                    analysis = {"raw_response": response_text}
            except json.JSONDecodeError:
                analysis = {"raw_response": response_text}
            
            logger.info(f"DÃ©tection anomalies: status={analysis.get('overall_status', 'unknown')}")
            
            return {
                "success": True,
                "metrics": metrics,
                "analysis": analysis,
                "model_used": self.model,
                "method": "ai"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _basic_anomaly_detection(self, metrics: Dict) -> List[Dict]:
        """DÃ©tection d'anomalies basique (sans IA)"""
        anomalies = []
        
        # CPU load
        if "load_avg" in metrics and metrics.get("cpu_cores"):
            load_1m = float(metrics["load_avg"][0])
            cores = metrics["cpu_cores"]
            if load_1m > cores * 0.8:
                anomalies.append({
                    "type": "cpu",
                    "severity": "high" if load_1m > cores else "medium",
                    "description": f"Load average Ã©levÃ©: {load_1m} (cores: {cores})"
                })
        
        # MÃ©moire
        if "memory" in metrics:
            usage = metrics["memory"].get("usage_percent", 0)
            if usage > 90:
                anomalies.append({
                    "type": "memory",
                    "severity": "critical",
                    "description": f"MÃ©moire critique: {usage}%"
                })
            elif usage > 80:
                anomalies.append({
                    "type": "memory",
                    "severity": "high",
                    "description": f"MÃ©moire Ã©levÃ©e: {usage}%"
                })
        
        # Disque
        if "disk" in metrics:
            usage_str = metrics["disk"].get("usage_percent", "0%")
            usage = int(usage_str.replace("%", ""))
            if usage > 90:
                anomalies.append({
                    "type": "disk",
                    "severity": "critical",
                    "description": f"Disque critique: {usage}%"
                })
            elif usage > 80:
                anomalies.append({
                    "type": "disk",
                    "severity": "high",
                    "description": f"Disque Ã©levÃ©: {usage}%"
                })
        
        # GPU
        if "gpu" in metrics:
            gpu_util = metrics["gpu"].get("utilization_percent", 0)
            gpu_mem = metrics["gpu"].get("memory_used_mb", 0) / max(metrics["gpu"].get("memory_total_mb", 1), 1) * 100
            
            if gpu_mem > 95:
                anomalies.append({
                    "type": "gpu",
                    "severity": "high",
                    "description": f"VRAM proche de la saturation: {gpu_mem:.1f}%"
                })
        
        return anomalies
    
    # === 3. GENERATE SUGGESTIONS ===
    def self_improve_suggestions(self, context: Optional[str] = None) -> Dict[str, Any]:
        """
        GÃ©nÃ©rer des suggestions d'amÃ©lioration
        
        Args:
            context: Contexte additionnel pour les suggestions
            
        Returns:
            Dict avec les suggestions d'amÃ©lioration
        """
        try:
            # Collecter les informations
            metrics = self._get_system_metrics()
            docker_logs = self._get_recent_logs("docker", lines=100)
            
            # Construire le prompt
            system_prompt = """Tu es un consultant DevOps expert.
Propose des amÃ©liorations pour optimiser l'infrastructure.

CatÃ©gories:
- Performance (CPU, mÃ©moire, GPU)
- SÃ©curitÃ© (ports, accÃ¨s, logs)
- Maintenance (backups, mises Ã  jour)
- Architecture (conteneurs, services)

RÃ©ponds en JSON:
{
  "suggestions": [
    {
      "category": "...",
      "priority": "low|medium|high",
      "title": "...",
      "description": "...",
      "action": "commande ou Ã©tape Ã  suivre",
      "impact": "..."
    }
  ],
  "quick_wins": ["actions rapides Ã  fort impact"],
  "long_term": ["amÃ©liorations Ã  planifier"]
}"""
            
            context_text = f"\n\nContexte additionnel: {context}" if context else ""
            
            prompt = f"""Analyse cette infrastructure et propose des amÃ©liorations:{context_text}

MÃ‰TRIQUES:
{json.dumps(metrics, indent=2)}

LOGS DOCKER (extrait):
{docker_logs[:5000]}

GÃ©nÃ¨re des suggestions d'amÃ©lioration."""
            
            # Appeler Ollama
            result = self._call_ollama(prompt, system_prompt)
            
            if not result["success"]:
                return result
            
            # Parser la rÃ©ponse
            response_text = result["response"]
            
            try:
                json_match = re.search(r'\{[\s\S]*\}', response_text)
                if json_match:
                    suggestions = json.loads(json_match.group())
                else:
                    suggestions = {"raw_response": response_text}
            except json.JSONDecodeError:
                suggestions = {"raw_response": response_text}
            
            # Sauvegarder les suggestions
            suggestions_path = LOGS_DIR / "suggestions" / f"suggestions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            suggestions_path.parent.mkdir(parents=True, exist_ok=True)
            suggestions_path.write_text(json.dumps(suggestions, indent=2, ensure_ascii=False))
            
            logger.info(f"Suggestions gÃ©nÃ©rÃ©es: {len(suggestions.get('suggestions', []))} items")
            
            return {
                "success": True,
                "suggestions": suggestions,
                "model_used": self.model,
                "report_path": str(suggestions_path)
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    # === MÃ©thodes utilitaires ===
    
    def get_analysis_history(self, limit: int = 10) -> Dict[str, Any]:
        """RÃ©cupÃ©rer l'historique des analyses"""
        return {
            "success": True,
            "history": self.analysis_history[-limit:],
            "total": len(self.analysis_history)
        }
    
    def test_ollama_connection(self) -> Dict[str, Any]:
        """Tester la connexion Ã  Ollama"""
        try:
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=5)
            
            if response.status_code == 200:
                models = response.json().get("models", [])
                return {
                    "success": True,
                    "connected": True,
                    "host": self.ollama_host,
                    "models": [m["name"] for m in models],
                    "configured_model": self.model
                }
            else:
                return {
                    "success": False,
                    "error": f"HTTP {response.status_code}"
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "host": self.ollama_host
            }


# Instance singleton
self_improve_tools = SelfImproveTools()
